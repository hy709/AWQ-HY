import re
import time
import torch
import logging
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model(model_path, dtype="float16"):
    """åŠ è½½æ¨¡å‹å¹¶è‡ªåŠ¨åˆ†é…è®¾å¤‡"""
    torch_dtype = torch.float16 if dtype == "float16" else torch.bfloat16
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def model_out(model, tokenizer, input_text, return_activations=False, max_length=5000):
    """
    è·å–æ¨¡å‹å“åº”ï¼ˆç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§ï¼‰
    """
    # ç¡®ä¿è¾“å…¥æ•°æ®ä¸æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    if return_activations:
        layer_activations = []

        def hook_fn(_, __, output):
            layer_activations.append(output[0].detach().cpu())

        hooks = [layer.register_forward_hook(hook_fn)
                 for layer in model.model.layers]

        with torch.no_grad():
            model(**inputs)

        for hook in hooks:
            hook.remove()

        return layer_activations
    else:
        with torch.no_grad():
            outputs = model.generate(
                max_length=max_length,
                repetition_penalty=1.5,
                no_repeat_ngram_size=3,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                **inputs
            )
            return tokenizer.decode(outputs[0], skip_special_tokens=True)

@torch.no_grad()
def get_act_scale(x):
    """è·å–æ¿€æ´»åˆ†å¸ƒ"""
    return x.abs().view(-1, x.shape[-1]).mean(0)

def pseudo_quantize_tensor(w, n_bit=8, zero_point=True, q_group_size=-1):
    """ä¼ªé‡åŒ–å¼ é‡ï¼ˆä¿æŒè®¾å¤‡ä¸€è‡´æ€§ï¼‰"""
    org_device = w.device
    org_shape = w.shape

    if q_group_size > 0:
        w = w.view(-1, q_group_size)

    if zero_point:
        max_val = w.amax(dim=1, keepdim=True)
        min_val = w.amin(dim=1, keepdim=True)
        scales = (max_val - min_val).clamp(min=1e-5) / (2 ** n_bit - 1)
        zeros = (-torch.round(min_val / scales)).clamp_(0, 2 ** n_bit - 1)
    else:
        max_val = w.abs().amax(dim=1, keepdim=True).clamp(min=1e-5)
        scales = max_val / (2 ** (n_bit - 1) - 1)
        zeros = torch.zeros_like(scales)

    w = (torch.clamp(torch.round(w / scales) + zeros, 0, 2 ** n_bit - 1) - zeros) * scales
    return w.view(org_shape).to(org_device)

def w_quantize_func(weight, scales, n_bit=8, zero_point=True):
    """æƒé‡é‡åŒ–å‡½æ•°ï¼ˆè®¾å¤‡æ„ŸçŸ¥ï¼‰"""
    # ç¡®ä¿scalesä¸weightåœ¨åŒä¸€è®¾å¤‡
    scales = scales.view(1, -1).to(weight.device)
    weight.data = pseudo_quantize_tensor(
        weight.data * scales,
        n_bit=n_bit,
        zero_point=zero_point
    )
    return weight

def judge(model, tokenizer, history_out, out):
    """è¯„åˆ¤é‡åŒ–ç»“æœ"""
    scores = []
    logger.info(f"è¯„ä¼°{len(history_out)}ä¸ªç­”æ¡ˆ")

    for idx, answer in enumerate(history_out):
        prompt = f"""
        è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è¦æ±‚è¯„ä¼°ï¼ˆå›ç­”åªéœ€åŒ…å«åˆ†æ•°ï¼‰ï¼š
        1. è¯„åˆ†æ ‡å‡†ï¼ˆæ€»åˆ†30åˆ†ï¼‰ï¼š
           - æ­£ç¡®æ€§ï¼ˆ10åˆ†ï¼‰ 
           - å®Œæ•´æ€§ï¼ˆ8åˆ†ï¼‰
           - é€»è¾‘ç»“æ„ï¼ˆ7åˆ†ï¼‰ 
           - ä¸“ä¸šæ€§ï¼ˆ5åˆ†ï¼‰
        2. è¾“å‡ºæ ¼å¼ï¼ˆä»…ä¸€è¡Œï¼‰ï¼š
        æ€»åˆ†=XX

        åŸå§‹å›ç­”ï¼š{out}
        é‡åŒ–å›ç­”ï¼š{answer}
        """

        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            output = model.generate(
                max_length=10000,
                generation_config=GenerationConfig(max_time=30),
                **inputs
            )
            response = tokenizer.decode(output[0], skip_special_tokens=True)

            match = re.search(r'æ€»åˆ†=(\d+)', response)
            score = int(match.group(1)) if match else 0
            scores.append(min(max(score, 0), 30))

        except Exception as e:
            logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
            scores.append(0)

    return scores

def awq_h(model_path, judge_model, judge_tokenizer, input_text, divide_num=4, n_bit=4):
    """AWQé‡åŒ–ä¸»å‡½æ•°"""
    model, tokenizer = load_model(model_path, dtype="float16")
    layers = model.model.layers
    # but å…¶å®å¦‚æœè€ƒè™‘åœ¨æ¯è¿‡å®Œä¸€æ¬¡ n_grid, å°±è€ƒè™‘ä¸€ä¸‹æ¿€æ´»å€¼ç„¶åå†åšä¸‹ä¸€è½®çš„ n_grid ä¼šä¸ä¼šæ›´å¥½ä¸€ç‚¹
    layer_activations = model_out(model, tokenizer, input_text, return_activations=True)

    logger.info(f"æ¨¡å‹å…±{len(layers)}å±‚ï¼Œåˆ†{divide_num}ç»„")
    org_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
    best_scales = []
    group_size = len(layers) // divide_num
    org_out = model_out(model, tokenizer, input_text)

    # check ä¸€ä¸‹åŸå§‹è¾“å‡º
    logger.info("åŸå§‹æ¨¡å‹è¾“å‡º: {}".format(org_out))

    for group_idx in tqdm(range(divide_num), desc="é‡åŒ–è¿›åº¦"):
        start = group_idx * group_size
        end = start + group_size - 1

        x = layer_activations[end]
        x_max = get_act_scale(x)
        history_out = []
        history_scales = {}

        # è€ƒè™‘æ¸è¿›é‡åŒ–
        group_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}

        n_grid = 20
        check_point = 5

        for ratio_idx in range(n_grid):
            if (ratio_idx + 1) % check_point == 0:
                logger.info(f"  - ç»„ {group_idx + 1} æ¯”ä¾‹ {ratio_idx / n_grid:.2f}")
            model.load_state_dict(group_state)

            # if group_idx == divide_num - 1 and ratio_idx == n_grid - 1:
            #     quant_out = model_out(model, tokenizer, input_text)
            #     logger.info("æœ€åä¸€å±‚çš„é‡åŒ–æ¨¡å‹è¾“å‡º: {}".format(quant_out))

            ratio = ratio_idx * 1.0 / n_grid
            scales = (x_max.pow(ratio).clamp(min=1e-4) / (x_max.max() * x_max.min()).sqrt()).view(-1)
            history_scales[ratio] = scales

            for layer_idx in range(start, end + 1):
                layer = model.model.layers[layer_idx]
                for name, param in layer.named_parameters():
                    if "weight" in name and len(param.shape) > 1 and param.shape[-1] == len(scales):
                        w_quantize_func(param.data, scales, n_bit)

            history_out.append(model_out(model, tokenizer, input_text))
            torch.cuda.empty_cache()

        answer = True
        for i in range(len(history_out)-1):
            if history_out[i] !=history_out[i+1]:
                answer = False
        logger.info("é‡åŒ–æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç›¸ç­‰: {}".format(answer))

        scores = judge(judge_model, judge_tokenizer, history_out, org_out)
        best_ratio = list(history_scales.keys())[scores.index(max(scores))]
        best_scales.append(history_scales[best_ratio])

        logger.info(f"ç¬¬{group_idx + 1}ç»„è¯„åˆ†ç»“æœ:")
        for ratio, score in zip(history_scales.keys(), scores):
            logger.info(f"  - æ¯”ä¾‹ {ratio:.2f}: {score}/30")
        logger.info(f"  => é€‰æ‹©æœ€ä½³æ¯”ä¾‹: {best_ratio:.3f}")

    # è‡ªç”±é€‰æ‹©æ˜¯å¦æ¢å¤æ¨¡å‹çŠ¶æ€ï¼Œä¸éœ€è¦å°±ç»™å®ƒæ³¨é‡Šæ‰
    # model.load_state_dict(org_state)

    # ä¼˜åŒ–best_scalesçš„è¾“å‡ºæ˜¾ç¤º
    formatted_scales = []
    for i, scale in enumerate(best_scales):
        if isinstance(scale, torch.Tensor):
            scale = scale.cpu().numpy()
        formatted_scales.append({
            "group": i + 1,
            "shape": scale.shape if hasattr(scale, 'shape') else len(scale),
            "min": float(scale.min()),
            "max": float(scale.max()),
            "mean": float(scale.mean()),
            "dtype": str(scale.dtype) if hasattr(scale, 'dtype') else type(scale).__name__
        })

    logger.info("é‡åŒ–ç»“æœç»Ÿè®¡:")
    for s in formatted_scales:
        logger.info(
            f"ç»„ {s['group']}: å½¢çŠ¶={s['shape']}, èŒƒå›´=[{s['min']:.4f}, {s['max']:.4f}], å‡å€¼={s['mean']:.4f}, ç±»å‹={s['dtype']}")

    return [s.cpu().tolist() for s in best_scales]


if __name__ == "__main__":
    try:
        logger.info("ğŸš€ å¼€å§‹é‡åŒ–æµç¨‹")
        start_time = time.time()

        model_path = '/root/autodl-tmp/Qwen2.5-0.5B-Instruct'
        judge_model_path = '/root/autodl-fs/Qwen2.5-7B-Instruct'
        input_text = 'ä»€ä¹ˆæ˜¯å“ˆå¸Œè¡¨ï¼Ÿå®ƒçš„å·¥ä½œåŸç†å’Œå¸¸è§åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ'

        judge_model, judge_tokenizer = load_model(judge_model_path, dtype="float16")
        result = awq_h(model_path, judge_model, judge_tokenizer, input_text, divide_num=4, n_bit=2)

        logger.info(f"âœ… é‡åŒ–å®Œæˆï¼è€—æ—¶: {time.time() - start_time:.1f}s")
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯: {e}")
        raise